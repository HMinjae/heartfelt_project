{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘은 떡볶이를 먹었지만 체했다. 하지만 집에 와서 약을 먹고 다 나아서 괜찮아졌다.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "0.5001906156539917\n",
      "50.02% 확률로 기분 안 좋음입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "#전처리 함수 만들기\n",
    "def preprocessing(review, okt, remove_stopwords = False, stop_words =[]):\n",
    "\n",
    "  # 1. 한글 및 공백 제외한 문자 모두 제거\n",
    "  review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]','',review)\n",
    "\n",
    "  #2. okt 객체를 활용하여 형태소 단어로 나눔\n",
    "  word_review = okt.morphs(review_text,stem=True)\n",
    "\n",
    "  if remove_stopwords:\n",
    "    #3. 불용어 제거(선택)\n",
    "    word_review = [token for token in word_review if not token in stop_words]\n",
    "  return word_review\n",
    "\n",
    "# 전체 텍스트 전처리\n",
    "stop_words = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한']\n",
    "okt = Okt()\n",
    "clean_train_review = []\n",
    "test_data = pd.read_csv('https://github.com/HMinjae/heartfelt_project/raw/main/TF_train.txt', header = 0, delimiter='\\t', quoting=3)\n",
    "\n",
    "clean_test_review = []\n",
    "for review in test_data['document']:\n",
    "  if type(review) == str:\n",
    "    clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
    "  else:\n",
    "    clean_test_review.append([])\n",
    "\n",
    "# 인덱스 벡터 변환 후 일정 길이 넘어가거나 모자라는 리뷰 패딩처리\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_review)\n",
    "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
    "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
    "\n",
    "word_vocab = tokenizer.word_index #단어사전형태\n",
    "MAX_SEQUENCE_LENGTH = 1000 #문장 최대 길이\n",
    "\n",
    "# nsmc_train_input.npy 파일 다운로드\n",
    "response = requests.get('https://github.com/HMinjae/heartfelt_project/raw/main/DATA/CLEAN_DATA/nsmc_train_input.npy')\n",
    "train_input = np.load(BytesIO(response.content))\n",
    "train_input = pad_sequences(train_input, maxlen=train_input.shape[1])\n",
    "\n",
    "# nsmc_train_label.npy 파일 다운로드\n",
    "response = requests.get('https://github.com/HMinjae/heartfelt_project/raw/main/DATA/CLEAN_DATA/nsmc_train_label.npy')\n",
    "train_label = np.load(BytesIO(response.content))\n",
    "\n",
    "# `data_configs.json` 파일 다운로드\n",
    "response = requests.get('https://github.com/HMinjae/heartfelt_project/raw/main/DATA/CLEAN_DATA/data_configs.json')\n",
    "prepro_configs = json.loads(response.text)\n",
    "\n",
    "model_name= 'cnn_classifier_kr'\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "\n",
    "kargs={'model_name': model_name, 'vocab_size':prepro_configs['vocab_size'],'embbeding_size':128, 'num_filters':100,'dropout_rate':0.5, 'hidden_dimension':250,'output_dimension':1}\n",
    "\n",
    "class CNNClassifier(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, **kargs):\n",
    "    super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "    self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])\n",
    "    self.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid',activation = tf.keras.activations.relu,\n",
    "                                    kernel_constraint = tf.keras.constraints.MaxNorm(max_value=3)) for kernel_size in [3,4,5]]\n",
    "    self.pooling = layers.GlobalMaxPooling1D()\n",
    "    self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
    "    self.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
    "                            activation = tf.keras.activations.relu,\n",
    "                            kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "    self.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
    "                            activation=tf.keras.activations.sigmoid,\n",
    "                            kernel_constraint= tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "\n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.embedding(x)\n",
    "    x = self.dropout(x)\n",
    "    x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis = 1)\n",
    "    x = self.fc1(x)\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n",
    "okt = Okt()\n",
    "tokenizer  = Tokenizer()\n",
    "\n",
    "\n",
    "urla = 'https://raw.githubusercontent.com/HMinjae/heartfelt_project/main/DATA/CLEAN_DATA/data_configs.json'\n",
    "\n",
    "# URL에서 데이터를 가져옵니다.\n",
    "response = requests.get(urla)\n",
    "\n",
    "# 응답을 JSON으로 파싱합니다.\n",
    "prepro_configs = json.loads(response.text)\n",
    "prepro_configs['vocab'] = word_vocab\n",
    "\n",
    "tokenizer.fit_on_texts(word_vocab)\n",
    "\n",
    "MAX_LENGTH = 100 #문장최대길이\n",
    "\n",
    "sentence = input('일기를 입력해주세요.: ')\n",
    "print(sentence)\n",
    "sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
    "stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'] # 불용어 추가할 것이 있으면 이곳에 추가\n",
    "sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
    "vector  = tokenizer.texts_to_sequences(sentence)\n",
    "pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
    "\n",
    "model = CNNClassifier(**kargs)\n",
    "predictions = model.predict(pad_new)\n",
    "predictions = float(predictions.squeeze(-1)[1])\n",
    "print(predictions)\n",
    "if(predictions > 0.8):\n",
    "  print(\"{:.2f}% 확률로 기분 매우 좋음입니다.\\n\".format(predictions * 100))\n",
    "elif(predictions > 0.6):\n",
    "  print(\"{:.2f}% 확률로 기분 좋음입니다.\\n\".format(predictions * 100))\n",
    "elif(predictions > 0.4):\n",
    "  print(\"{:.2f}% 확률로 기분 안 좋음입니다.\\n\".format(predictions * 100))\n",
    "elif(predictions > 0.2):\n",
    "  print(\"{:.2f}% 확률로 보통입니다.\\n\".format(predictions * 100))\n",
    "else:\n",
    "  print(\"{:.2f}% 확률로 기분 매우 안 좋음입니다.\\n\".format((predictions) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#감정답변\n",
    "df = pd.read_csv('https://github.com/HMinjae/heartfelt_project/raw/main/%EA%B0%90%EC%A0%95%EB%8B%B5%EB%B3%80.csv')  \n",
    "\n",
    "if predictions >= 0.8:\n",
    "    print(random.choice(df.iloc[:, 0].dropna().tolist()))\n",
    "elif predictions >= 0.6:\n",
    "    print(random.choice(df.iloc[:, 1].dropna().tolist()))\n",
    "elif predictions >= 0.4:\n",
    "    print(random.choice(df.iloc[:, 2].dropna().tolist()))\n",
    "elif predictions >= 0.2:\n",
    "    print(random.choice(df.iloc[:, 3].dropna().tolist()))\n",
    "else:\n",
    "    print(random.choice(df.iloc[:, 4].dropna().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#노래추천\n",
    "df = pd.read_csv('노래추천.csv')  \n",
    "\n",
    "if predictions >= 0.8:\n",
    "    print(random.choice(df.iloc[:, 0].dropna().tolist()))\n",
    "elif predictions >= 0.6:\n",
    "    print(random.choice(df.iloc[:, 1].dropna().tolist()))\n",
    "elif predictions >= 0.4:\n",
    "    print(random.choice(df.iloc[:, 2].dropna().tolist()))\n",
    "elif predictions >= 0.2:\n",
    "    print(random.choice(df.iloc[:, 3].dropna().tolist()))\n",
    "else:\n",
    "    print(random.choice(df.iloc[:, 4].dropna().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#행동추천\n",
    "df = pd.read_csv('행동추천.csv')  \n",
    "if predictions >= 0.4:\n",
    "    print(random.choice(df.iloc[:, 0].dropna().tolist()))\n",
    "else:\n",
    "    print(random.choice(df.iloc[:, 4].dropna().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

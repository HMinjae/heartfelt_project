{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Le4Bb4cHRMjF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUA7nZ_VRb9x",
        "outputId": "184ae31c-3265-4d9e-90bb-ab1781f3fda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in c:\\users\\hmj01\\anaconda3\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\hmj01\\anaconda3\\lib\\site-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\hmj01\\anaconda3\\lib\\site-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in c:\\users\\hmj01\\anaconda3\\lib\\site-packages (from konlpy) (1.24.3)\n",
            "Requirement already satisfied: packaging in c:\\users\\hmj01\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (23.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tq9LhX1LRg70"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "train_data = pd.read_csv('https://github.com/HMinjae/heartfelt_project/raw/main/TF_train.txt', header = 0, delimiter='\\t', quoting=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YMIzejo_RkIc"
      },
      "outputs": [],
      "source": [
        "#전처리 함수 만들기\n",
        "def preprocessing(review, okt, remove_stopwords = False, stop_words =[]):\n",
        "  #함수인자설명\n",
        "  # review: 전처리할 텍스트\n",
        "  # okt: okt객체를 반복적으로 생성하지 않고 미리 생성 후 인자로 받음\n",
        "  # remove_stopword: 불용어를 제거할지 여부 선택. 기본값 False\n",
        "  # stop_words: 불용어 사전은 사용자가 직접 입력, 기본값 빈 리스트\n",
        "\n",
        "  # 1. 한글 및 공백 제외한 문자 모두 제거\n",
        "  review_text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]','',review)\n",
        "\n",
        "  #2. okt 객체를 활용하여 형태소 단어로 나눔\n",
        "  word_review = okt.morphs(review_text,stem=True)\n",
        "\n",
        "  if remove_stopwords:\n",
        "    #3. 불용어 제거(선택)\n",
        "    word_review = [token for token in word_review if not token in stop_words]\n",
        "  return word_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIL8BMpHRmgE",
        "outputId": "43e69b01-3cbd-4260-c806-4006ac59b052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['오늘', '하루', '도', '너무', '잘', '하다'],\n",
              " ['항상', '긍정', '적', '인', '마음', '당신', '을', '빛나다', '해', '요'],\n",
              " ['당신', '미소', '주변', '을', '환하다', '만들다'],\n",
              " ['매', '순간', '소중하다', '추억', '으로', '남', '을', '거', '예요']]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 전체 텍스트 전처리\n",
        "stop_words = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한']\n",
        "okt = Okt()\n",
        "clean_train_review = []\n",
        "\n",
        "for review in train_data['document']:\n",
        "  # 리뷰가 문자열인 경우만 전처리 진행\n",
        "  if type(review) == str:\n",
        "    clean_train_review.append(preprocessing(review,okt,remove_stopwords=True,stop_words= stop_words))\n",
        "  else:\n",
        "    clean_train_review.append([]) #str이 아닌 행은 빈칸으로 놔두기\n",
        "\n",
        "clean_train_review[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mf2yq-dxRo1E"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv('https://github.com/HMinjae/heartfelt_project/raw/main/TF_train.txt', header = 0, delimiter='\\t', quoting=3)\n",
        "\n",
        "clean_test_review = []\n",
        "for review in test_data['document']:\n",
        "  if type(review) == str:\n",
        "    clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))\n",
        "  else:\n",
        "    clean_test_review.append([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Hy1FwPdWRutO"
      },
      "outputs": [],
      "source": [
        "# 인덱스 벡터 변환 후 일정 길이 넘어가거나 모자라는 리뷰 패딩처리\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_train_review)\n",
        "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
        "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
        "\n",
        "word_vocab = tokenizer.word_index #단어사전형태\n",
        "MAX_SEQUENCE_LENGTH = 1000 #문장 최대 길이\n",
        "\n",
        "#학습 데이터\n",
        "train_inputs = pad_sequences(train_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
        "\n",
        "#학습 데이터 라벨 벡터화\n",
        "train_labels = np.array(train_data['label'])\n",
        "\n",
        "#평가 데이터\n",
        "test_inputs = pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
        "#평가 데이터 라벨 벡터화\n",
        "test_labels = np.array(test_data['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1phcBuzARwaS"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터 불러오기\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7E-16KK9Ryup"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# nsmc_train_input.npy 파일 다운로드\n",
        "response = requests.get('https://github.com/HMinjae/heartfelt_project/raw/main/DATA/CLEAN_DATA/nsmc_train_input.npy')\n",
        "train_input = np.load(BytesIO(response.content))\n",
        "train_input = pad_sequences(train_input, maxlen=train_input.shape[1])\n",
        "\n",
        "# nsmc_train_label.npy 파일 다운로드\n",
        "response = requests.get('https://github.com/HMinjae/heartfelt_project/raw/main/DATA/CLEAN_DATA/nsmc_train_label.npy')\n",
        "train_label = np.load(BytesIO(response.content))\n",
        "\n",
        "# `data_configs.json` 파일 다운로드\n",
        "response = requests.get('https://github.com/HMinjae/heartfelt_project/raw/main/DATA/CLEAN_DATA/data_configs.json')\n",
        "prepro_configs = json.loads(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GI5k_W2rR1ix"
      },
      "outputs": [],
      "source": [
        "model_name= 'cnn_classifier_kr'\n",
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 10\n",
        "VALID_SPLIT = 0.1\n",
        "MAX_LEN = train_input.shape[1]\n",
        "\n",
        "kargs={'model_name': model_name, 'vocab_size':prepro_configs['vocab_size'],'embbeding_size':128, 'num_filters':100,'dropout_rate':0.5, 'hidden_dimension':250,'output_dimension':1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W-n5SeB3R4Wj"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, **kargs):\n",
        "    super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
        "    self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])\n",
        "    self.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid',activation = tf.keras.activations.relu,\n",
        "                                    kernel_constraint = tf.keras.constraints.MaxNorm(max_value=3)) for kernel_size in [3,4,5]]\n",
        "    self.pooling = layers.GlobalMaxPooling1D()\n",
        "    self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
        "    self.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
        "                            activation = tf.keras.activations.relu,\n",
        "                            kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
        "    self.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
        "                            activation=tf.keras.activations.sigmoid,\n",
        "                            kernel_constraint= tf.keras.constraints.MaxNorm(max_value=3.))\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.dropout(x)\n",
        "    x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis = 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eeZIb9KyR8oa"
      },
      "outputs": [],
      "source": [
        "model = CNNClassifier(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qMX-rnxjR-Ds"
      },
      "outputs": [],
      "source": [
        "INPUT_TEST_DATA = 'nsmc_test_input.npy'\n",
        "LABEL_TEST_DATA = 'nsmc_test_label.npy'\n",
        "SAVE_FILE_NM = 'model.keras'\n",
        "\n",
        "\n",
        "# nsmc_train_input.npy 파일 다운로드\n",
        "response = requests.get('https://github.com/HMinjae/heartfelt_project/raw/main/DATA/CLEAN_DATA/nsmc_test_input.npy')\n",
        "test_input = np.load(BytesIO(response.content))\n",
        "test_input = pad_sequences(test_input, maxlen=test_input.shape[1])\n",
        "\n",
        "# nsmc_train_label.npy 파일 다운로드\n",
        "response = requests.get('https://github.com/HMinjae/heartfelt_project/raw/main/DATA/CLEAN_DATA/nsmc_test_label.npy')\n",
        "test_label_data = np.load(BytesIO(response.content))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-SSTD2_Scog",
        "outputId": "d7564acb-fc1e-4d5c-ce60-74031d3faea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1874 - loss: 0.7048\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.692462146282196, 0.5073694586753845]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import requests\n",
        "\n",
        "# 모델 파일 다운로드\n",
        "url = 'https://github.com/HMinjae/heartfelt_project/raw/main/DATA/my_models/model.keras'\n",
        "r = requests.get(url)\n",
        "open('model.keras', 'wb').write(r.content)\n",
        "\n",
        "# 모델 로드\n",
        "model = model = CNNClassifier(**kargs)\n",
        "\n",
        "# 모델 컴파일 (필요시)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])\n",
        "\n",
        "# 평가 실행\n",
        "model.evaluate(test_input, test_label_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBDr_Ao0SI97",
        "outputId": "2594db4a-33d7-432e-dca8-303200904b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "나는 오늘 치킨을 먹었다\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "0.4879351556301117\n",
            "48.79% 확률로 기분 안 좋음입니다.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "okt = Okt()\n",
        "tokenizer  = Tokenizer()\n",
        "\n",
        "\n",
        "urla = 'https://raw.githubusercontent.com/HMinjae/heartfelt_project/main/DATA/CLEAN_DATA/data_configs.json'\n",
        "\n",
        "# URL에서 데이터를 가져옵니다.\n",
        "response = requests.get(urla)\n",
        "\n",
        "# 응답을 JSON으로 파싱합니다.\n",
        "prepro_configs = json.loads(response.text)\n",
        "prepro_configs['vocab'] = word_vocab\n",
        "\n",
        "tokenizer.fit_on_texts(word_vocab)\n",
        "\n",
        "MAX_LENGTH = 100 #문장최대길이\n",
        "\n",
        "sentence = input('일기를 입력해주세요.: ')\n",
        "print(sentence)\n",
        "sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\\\s ]','', sentence)\n",
        "stopwords = ['은','는','이','가','하','아','것','들','의','있','되','수','보','주','등','한'] # 불용어 추가할 것이 있으면 이곳에 추가\n",
        "sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "sentence = [word for word in sentence if not word in stopwords] # 불용어 제거\n",
        "vector  = tokenizer.texts_to_sequences(sentence)\n",
        "pad_new = pad_sequences(vector, maxlen = MAX_LENGTH) # 패딩\n",
        "\n",
        "model = model = CNNClassifier(**kargs)\n",
        "predictions = model.predict(pad_new)\n",
        "predictions = float(predictions.squeeze(-1)[1])\n",
        "print(predictions)\n",
        "if(predictions > 0.8):\n",
        "  print(\"{:.2f}% 확률로 기분 매우 좋음입니다.\\n\".format(predictions * 100))\n",
        "elif(predictions > 0.6):\n",
        "  print(\"{:.2f}% 확률로 기분 좋음입니다.\\n\".format(predictions * 100))\n",
        "elif(predictions > 0.4):\n",
        "  print(\"{:.2f}% 확률로 기분 안 좋음입니다.\\n\".format(predictions * 100))\n",
        "elif(predictions > 0.2):\n",
        "  print(\"{:.2f}% 확률로 보통입니다.\\n\".format(predictions * 100))\n",
        "else:\n",
        "  print(\"{:.2f}% 확률로 기분 매우 안 좋음입니다.\\n\".format((predictions) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "평범한 하루지만, 그 속에서도 즐거웠던 일, 보람 있었던 일 있었을 거야.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#감정답변\n",
        "df = pd.read_csv('https://github.com/HMinjae/heartfelt_project/raw/main/%EA%B0%90%EC%A0%95%EB%8B%B5%EB%B3%80.csv')  \n",
        "\n",
        "if predictions >= 0.8:\n",
        "    print(random.choice(df.iloc[:, 0].dropna().tolist()))\n",
        "elif predictions >= 0.6:\n",
        "    print(random.choice(df.iloc[:, 1].dropna().tolist()))\n",
        "elif predictions >= 0.4:\n",
        "    print(random.choice(df.iloc[:, 2].dropna().tolist()))\n",
        "elif predictions >= 0.2:\n",
        "    print(random.choice(df.iloc[:, 3].dropna().tolist()))\n",
        "else:\n",
        "    print(random.choice(df.iloc[:, 4].dropna().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "재지팩트 – 하루종일\n"
          ]
        }
      ],
      "source": [
        "#노래추천\n",
        "df = pd.read_csv('노래추천.csv')  \n",
        "\n",
        "if predictions >= 0.8:\n",
        "    print(random.choice(df.iloc[:, 0].dropna().tolist()))\n",
        "elif predictions >= 0.6:\n",
        "    print(random.choice(df.iloc[:, 1].dropna().tolist()))\n",
        "elif predictions >= 0.4:\n",
        "    print(random.choice(df.iloc[:, 2].dropna().tolist()))\n",
        "elif predictions >= 0.2:\n",
        "    print(random.choice(df.iloc[:, 3].dropna().tolist()))\n",
        "else:\n",
        "    print(random.choice(df.iloc[:, 4].dropna().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lobonabeat! – 빵댕이(Feat.노윤하)\n"
          ]
        }
      ],
      "source": [
        "#행동추천\n",
        "df = pd.read_csv('행동추천.csv')  \n",
        "if predictions >= 0.4:\n",
        "    print(random.choice(df.iloc[:, 0].dropna().tolist()))\n",
        "else:\n",
        "    print(random.choice(df.iloc[:, 4].dropna().tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
